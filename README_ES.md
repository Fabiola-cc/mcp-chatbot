# MCP Chatbot - Asistente de IA Multi-Servidor

Un sistema de chatbot que integra m√∫ltiples servidores del Protocolo de Contexto de Modelo (MCP) con modelos de lenguaje tanto locales (Ollama) como en la nube (Anthropic Claude). Este proyecto proporciona una interfaz unificada para interactuar con varios servicios especializados incluyendo coaching de sue√±o, recomendaciones de belleza, informaci√≥n de videojuegos, gesti√≥n de archivos y m√°s.

## ‚ú® Caracter√≠sticas

- **Soporte Dual de LLM**: Elige entre privacidad local con Ollama o capacidades avanzadas con Anthropic Claude
- **M√∫ltiples Servidores MCP**: Soporte integrado para 6+ servidores especializados
- **Gesti√≥n de Sesiones**: Historial de conversaciones persistente con gesti√≥n de contexto
- **Logging Integral**: Seguimiento detallado de interacciones y monitoreo de rendimiento
- **Soporte de Servidor Remoto**: Conecta a servidores MCP tanto locales como remotos
- **Interfaz de Comandos**: Comandos especiales para gesti√≥n del sistema y depuraci√≥n

## üèóÔ∏è Arquitectura

```
src/chatbot/
‚îú‚îÄ‚îÄ clients/
‚îÇ   ‚îú‚îÄ‚îÄ ollama_client.py          # Cliente LLM local (enfocado en privacidad)
‚îÇ   ‚îú‚îÄ‚îÄ anthropic_client.py       # Cliente API Claude (basado en nube)
‚îÇ   ‚îú‚îÄ‚îÄ connection.py             # Conexi√≥n gen√©rica servidor MCP
‚îÇ   ‚îî‚îÄ‚îÄ remote_client.py          # Cliente servidor MCP remoto
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ session_manager.py        # Gesti√≥n historial conversaciones
‚îÇ   ‚îî‚îÄ‚îÄ logger.py                 # Logging interacciones y anal√≠ticas
‚îú‚îÄ‚îÄ main.py                       # Punto entrada chatbot basado en Ollama
‚îî‚îÄ‚îÄ main_anthropic.py             # Punto entrada chatbot basado en Claude
```

## üéØ Servidores MCP Soportados

| Servidor | Descripci√≥n | Tipo | Caracter√≠sticas |
|----------|-------------|------|-----------------|
| **Sleep Coach** | Consejos higiene sue√±o y bienestar | Local | Recomendaciones personalizadas de sue√±o |
| **Beauty Palette** | Recomendaciones belleza y cosm√©ticos | Local | Combinaci√≥n colores, sugerencias productos |
| **Video Games** | Informaci√≥n y recomendaciones juegos | Local | B√∫squeda juegos, rese√±as, recomendaciones |
| **Movies** | Base datos pel√≠culas y recomendaciones | Local | B√∫squeda films, calificaciones, sugerencias |
| **Git** | Operaciones control de versiones | Local | Gesti√≥n repositorios, commits |
| **Filesystem** | Operaciones archivos y directorios | Local | Lectura/escritura archivos, navegaci√≥n |
| **Sleep Quotes** | Contenido inspiracional relacionado sue√±o | Remoto | Sabidur√≠a diaria, recordatorios dormir |

## üöÄ Inicio R√°pido

### Prerrequisitos

- Python 3.8+
- Node.js (para servidor filesystem)
- Git
- **Para Ollama**: [Instalaci√≥n Ollama](https://ollama.com/)
- **Para Claude**: Clave API Anthropic

### Instalaci√≥n

1. **Clonar el repositorio**
```bash
git clone https://github.com/Fabiola-cc/mcp-chatbot.git
cd mcp-chatbot
```

2. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

3. **Configurar variables de entorno**
```bash
# Copiar plantilla entorno
cp .env.example .env

# Editar archivo .env con tus configuraciones
# Para API Claude (opcional):
ANTHROPIC_API_KEY=tu_clave_api_aqu√≠
```

### Opci√≥n 1: Configuraci√≥n Local con Ollama (Recomendado para Privacidad)

4. **Instalar y configurar Ollama**
```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Iniciar servicio Ollama
ollama serve

# Descargar un modelo (en otra terminal)
ollama pull llama3.2:3b
```

5. **Ejecutar el chatbot**
```bash
cd src/chatbot
python main.py
```

### Opci√≥n 2: Configuraci√≥n Nube con Anthropic Claude

4. **Configurar clave API**
- Obt√©n tu clave API desde [Consola Anthropic](https://console.anthropic.com/)
- Agr√©gala a tu archivo `.env`

5. **Ejecutar el chatbot**
```bash
cd src/chatbot
python main_anthropic.py
```

## üí¨ Uso

### Comandos B√°sicos

Una vez que el chatbot est√© ejecut√°ndose, puedes:

- **Chatear normalmente**: Hacer preguntas, solicitar recomendaciones, o buscar consejos
- **Usar comandos especiales**: Escribir comandos que inicien con `/`

### Comandos Especiales

| Comando | Descripci√≥n |
|---------|-------------|
| `/help` | Mostrar comandos disponibles |
| `/stats` | Mostrar estad√≠sticas de sesi√≥n |
| `/log` | Ver logs de interacciones |
| `/context` | Mostrar contexto conversaci√≥n |
| `/clear` | Limpiar historial conversaci√≥n |
| `/save` | Guardar sesi√≥n actual |
| `/quit` | Salir del chatbot |

### Ejemplos de Interacciones

```
üí§ T√∫: Necesito ayuda con mi horario de sue√±o

ü§ñ Chatbot: ¬°Puedo ayudarte con recomendaciones de sue√±o! Perm√≠teme obtener
algunos consejos personalizados para ti.

üí§ T√∫: Mu√©strame una cita motivacional sobre el sue√±o

ü§ñ Chatbot: [Llama al servidor Sleep Quotes]
üåô CITA INSPIRACIONAL PARA DORMIR üåô

"El sue√±o es la mejor meditaci√≥n que existe. Entr√©gate a √©l con gratitud."
‚Äî Dalai Lama

üí§ T√∫: ¬øQu√© juegos est√°n en tendencia ahora?

ü§ñ Chatbot: [Llama al servidor Video Games para tendencias actuales]
```

## üîß Configuraci√≥n

### Selecci√≥n de Modelo

**Modelos Ollama** (Local):
- `llama3.2:3b` - Ligero, buen rendimiento
- `qwen2.5:3b` - Excelente para espa√±ol
- `codellama:7b` - Especializado en c√≥digo

**Modelos Claude** (Nube):
- `claude-3-5-haiku-20241022` - R√°pido, eficiente
- `claude-3-5-sonnet-20241022` - Rendimiento equilibrado
- `claude-3-opus-20240229` - M√°xima capacidad

### Configuraci√≥n Sesi√≥n

```python
# En session_manager.py
session = SessionManager(
    max_context_messages=20  # Ajustar ventana contexto
)
```

### Configuraci√≥n Logging

```python
# En logger.py
logger = InteractionLogger(
    log_dir="logs",
    log_level="INFO"  # DEBUG, INFO, WARNING, ERROR
)
```

## üîå Agregar Nuevos Servidores MCP

1. **Crear implementaci√≥n servidor** en `servidores locales mcp/`
2. **Agregar conexi√≥n cliente** en `clients/`
3. **Registrar en main.py**:

```python
self.clients["tu_servidor"] = Client()

await self.clients["tu_servidor"].start_server(
    "tu_servidor", 
    sys.executable,
    "ruta/a/tu/servidor.py"
)
```

4. **Actualizar contexto LLM** con herramientas servidor

## üìä Monitoreo y Anal√≠ticas

El sistema proporciona monitoreo integral:

- **Estad√≠sticas Sesi√≥n**: Conteos mensajes, duraci√≥n, uso contexto
- **Interacciones MCP**: Tasas √©xito, uso servidores, seguimiento errores
- **M√©tricas Rendimiento**: Tiempos respuesta, uso tokens
- **Logging Detallado**: Historial completo interacciones con timestamps

## üêõ Soluci√≥n de Problemas

### Problemas Comunes

**Fall√≥ Conexi√≥n Ollama**
```bash
# Asegurar que Ollama est√© ejecut√°ndose
ollama serve

# Verificar si modelo est√° disponible
ollama list
```

**Error API Anthropic**
```bash
# Verificar clave API en .env
echo $ANTHROPIC_API_KEY

# Verificar validez clave API en console.anthropic.com
```

**Fall√≥ Inicio Servidor MCP**
```bash
# Verificar logs servidor
tail -f logs/interactions.log

# Verificar dependencias servidor
python servidores_locales_mcp/TuServidor/server.py
```

## ü§ù Contribuir

1. Fork el repositorio
2. Crear rama caracter√≠stica (`git checkout -b feature/caracter√≠stica-incre√≠ble`)
3. Commit tus cambios (`git commit -m 'Agregar caracter√≠stica incre√≠ble'`)
4. Push a la rama (`git push origin feature/caracter√≠stica-incre√≠ble`)
5. Abrir Pull Request

### Gu√≠as Desarrollo

- Seguir estructura c√≥digo existente y convenciones nomenclatura
- Agregar manejo errores integral
- Incluir logging para nuevas caracter√≠sticas
- Probar con configuraciones Ollama y Claude
- Actualizar documentaci√≥n para nuevos servidores MCP

## üìÑ Licencia

Este proyecto est√° licenciado bajo la Licencia MIT - ver archivo [LICENSE](LICENSE) para detalles.

## üôè Reconocimientos

- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) por la estandarizaci√≥n
- [Anthropic](https://www.anthropic.com/) por API Claude
- [Ollama](https://ollama.com/) por infraestructura LLM local
- Todos los contribuidores al ecosistema servidor MCP